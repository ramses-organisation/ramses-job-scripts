{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fbcff-66e0-4530-8196-716649ca6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# TODO: update db_path during building of STATIC_DB\n",
    "\n",
    "# Define the metadata structure\n",
    "metadata_structure = {\n",
    "    'cc_main_name': {'type': 'string', 'mandatory': True},\n",
    "    'cc_main_full': {'type': 'string', 'mandatory': False},\n",
    "    'cc_alt_name': {'type': 'string', 'mandatory': False},\n",
    "    'cc_alt_full': {'type': 'string', 'mandatory': False},\n",
    "    'sc_main_name': {'type': 'string', 'mandatory': True},\n",
    "    'sc_alt_name': {'type': 'string', 'mandatory': False},\n",
    "    'exec_year': {'type': 'int', 'mandatory': True},\n",
    "    'exec_date': {'type': 'string', 'mandatory': False},\n",
    "    'proj_main_name': {'type': 'string', 'mandatory': False},\n",
    "    'proj_alt_name': {'type': 'string', 'mandatory': False},\n",
    "    'pi_name': {'type': 'string', 'mandatory': True},\n",
    "    'pi_email': {'type': 'string', 'mandatory': False},\n",
    "    'sim_bibcode': {'type': 'string', 'mandatory': False},\n",
    "    'sim_queue': {'type': 'string', 'mandatory': True},\n",
    "    'sim_nnodes': {'type': 'int', 'mandatory': True},\n",
    "    'sim_nmpi': {'type': 'int', 'mandatory': True},\n",
    "    'sim_ncpu': {'type': 'int', 'mandatory': True},\n",
    "    'sim_ngpu': {'type': 'int', 'mandatory': False},\n",
    "    'sim_nthreads_total': {'type': 'int', 'mandatory': True},\n",
    "    'sim_nvector': {'type': 'int', 'mandatory': False},\n",
    "    'sim_cpu_compiler': {'type': 'string', 'mandatory': True},\n",
    "    'sim_accel_compiler': {'type': 'string', 'mandatory': False},\n",
    "    'sim_modules': {'type': 'string', 'mandatory': False},\n",
    "    'ori_file_name': {'type': 'string', 'mandatory': True},\n",
    "    'alt_file_name': {'type': 'string', 'mandatory': False},\n",
    "    'db_file_name': {'type': 'string', 'mandatory': False},\n",
    "    'db_path': {'type': 'string', 'mandatory': False}\n",
    "}\n",
    "\n",
    "def extract_metadata(file_path):\n",
    "    metadata = {key: ('' if value['type'] == 'string' else 0) for key, value in metadata_structure.items()}\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        header_match = re.search(r'########\\s*#HEADER\\s*########(.*?)#######\\s*#SCRIPT\\s*#######', content, re.DOTALL)\n",
    "        if header_match:\n",
    "            header = header_match.group(1)\n",
    "            lines = header.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                if '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    if key in metadata_structure:\n",
    "                        if metadata_structure[key]['type'] == 'int':\n",
    "                            try:\n",
    "                                metadata[key] = int(value) if value else 0\n",
    "                            except ValueError:\n",
    "                                print(f\"Warning: Could not convert '{value}' for key '{key}' to int in file {os.path.basename(file_path)}\")\n",
    "                        else:\n",
    "                            metadata[key] = value\n",
    "    return metadata\n",
    "\n",
    "def validate_metadata(metadata, file_name):\n",
    "    errors = []\n",
    "    for key, props in metadata_structure.items():\n",
    "        if props['mandatory'] and not metadata.get(key):\n",
    "            # Special handling for year, allowing fallback to exec_date\n",
    "            if key == 'exec_year' and metadata.get('exec_date'):\n",
    "                continue\n",
    "            errors.append(f\"Mandatory field '{key}' is missing or empty in file '{file_name}'\")\n",
    "    return errors\n",
    "\n",
    "def process_files(directory):\n",
    "    database = defaultdict(list)\n",
    "    file_metadata = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.sh', '.lsf')):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            metadata = extract_metadata(file_path)\n",
    "            errors = validate_metadata(metadata, filename)\n",
    "            if errors:\n",
    "                print(f\"Errors in file '{filename}':\")\n",
    "                for error in errors:\n",
    "                    print(f\"  - {error}\")\n",
    "            file_metadata[filename] = metadata\n",
    "            for key, value in metadata.items():\n",
    "                database[key].append(value)\n",
    "    return database, file_metadata\n",
    "\n",
    "def organize_files(file_metadata, source_dir, target_dir):\n",
    "    for filename, metadata in file_metadata.items():\n",
    "        cc_name = metadata.get('cc_main_name')\n",
    "        computer_name = metadata.get('sc_main_name')\n",
    "        compiler = metadata.get('sim_cpu_compiler')\n",
    "        exec_year = metadata.get('exec_year')\n",
    "        exec_date = metadata.get('exec_date')\n",
    "\n",
    "        # Determine execution year, prioritizing exec_year\n",
    "        if not exec_year:\n",
    "            if exec_date:\n",
    "                try:\n",
    "                    exec_year = datetime.strptime(exec_date, '%d-%m-%Y').year\n",
    "                except (ValueError, TypeError):\n",
    "                    exec_year = 'unknown'\n",
    "            else:\n",
    "                exec_year = 'unknown'\n",
    "        \n",
    "        if exec_year == 'unknown':\n",
    "            print(f\"Warning: Could not determine execution year for file {filename}. Using 'unknown'.\")\n",
    "\n",
    "        # Ensure essential components for path are present\n",
    "        if not all([cc_name, computer_name, exec_year, compiler]):\n",
    "            print(f\"Error: Cannot determine target directory for {filename} due to missing metadata. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create directory structure\n",
    "        dir_path = os.path.join(target_dir, cc_name, computer_name, str(exec_year), compiler)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        # Copy file to new location\n",
    "        source_file = os.path.join(source_dir, filename)\n",
    "        target_file = os.path.join(dir_path, filename)\n",
    "        shutil.copy2(source_file, target_file)\n",
    "        print(f\"Copied {filename} to {dir_path}\")\n",
    "\n",
    "def save_database_to_npz(database, output_file):\n",
    "    \"\"\"\n",
    "    Save the database to a .npz file.\n",
    "    \"\"\"\n",
    "    # np.savez cannot handle object arrays with mixed types like the 'files' dict.\n",
    "    # We create a clean version of the database for saving.\n",
    "    db_to_save = {k: v for k, v in database.items() if k != 'files'}\n",
    "    np.savez(output_file, **db_to_save)\n",
    "    print(f\"Database saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    incoming_directory = \"../../INCOMING\"\n",
    "    target_directory = \"../../STATIC_DB\"\n",
    "    npz_output_file = \"ramses_slurm_database.npz\"\n",
    "\n",
    "    database, file_metadata = process_files(incoming_directory)\n",
    "    \n",
    "    if file_metadata:\n",
    "        print(\"\\nComputing centers represented in the sample:\")\n",
    "        print(np.unique(database['cc_main_name']))\n",
    "\n",
    "        print(\"\\nAll supercomputer names:\")\n",
    "        print(np.unique(database['sc_main_name']))\n",
    "\n",
    "        print(\"\\nAll metadata:\")\n",
    "        for key in metadata_structure:\n",
    "            # to avoid printing the large list of file metadata\n",
    "            if key in database:\n",
    "                 print(f\"{key}: {database[key]}\")\n",
    "\n",
    "        print(\"\\nNumber of files processed:\", len(file_metadata))\n",
    "\n",
    "        print(\"\\nExample of complete metadata for one file:\")\n",
    "        example_file = next(iter(file_metadata))\n",
    "        print(f\"File: {example_file}\")\n",
    "        for key, value in file_metadata[example_file].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        # Organize files into static directory structure\n",
    "        organize_files(file_metadata, incoming_directory, target_directory)\n",
    "\n",
    "        print(\"\\nFiles have been organized into the static directory structure.\")\n",
    "\n",
    "        # Save the database to a .npz file\n",
    "        save_database_to_npz(database, npz_output_file)\n",
    "    else:\n",
    "        print(\"No script files found in the INCOMING directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6808f4d-f99c-4ee2-9891-d06e684855ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}